import asyncio

from luma.client import LumaClient


async def main():
    try:
        # Con Ollama
        client = LumaClient(
            base_url="http://0.0.0.0:1234",
            api_key="dev",
            use_ollama=True,
            ollama_embedding_model="embeddinggemma:300m",
            ollama_llm_model="nemotron-3-nano:30b-cloud",
        )

        print("Cliente Luma inicializado con Ollama")

        # Crear colecciÃ³n automÃ¡ticamente
        dimension = client.create_rag_collection("mi_coleccion")
        print(f"ColecciÃ³n creada con dimensiÃ³n: {dimension}")

        # Ingresar documento
        text = """
        Este es un documento de prueba sobre inteligencia artificial.
        La inteligencia artificial (IA) es la simulaciÃ³n de procesos de inteligencia humana
        por parte de mÃ¡quinas, especialmente sistemas informÃ¡ticos.

        Los principales tipos de IA incluyen:
        1. IA dÃ©bil o estrecha: DiseÃ±ada para tareas especÃ­ficas
        2. IA fuerte o general: Puede realizar cualquier tarea intelectual humana
        3. Superinteligencia: Excede la inteligencia humana en todos los aspectos

        Machine Learning es un subcampo de la IA que se centra en el desarrollo de
        algoritmos que permiten a las computadoras aprender de los datos.
        """

        chunks = client.ingest_document(
            collection="mi_coleccion", text=text, metadata={"author": "yo", "source": "documento", "tema": "IA"}
        )

        print(f"Documento ingresado en {chunks} chunks")

        # Hacer pregunta
        print("\nRealizando consulta RAG...")
        respuesta = client.ask(
            collection="mi_coleccion",
            question="Â¿QuÃ© es la inteligencia artificial y quÃ© tipos existen?",
            k=3,
            temperature=0.1,
        )

        print("\n" + "=" * 50)
        print("RESPUESTA:")
        print("=" * 50)
        print(respuesta.answer)
        print("\n" + "=" * 50)
        print(f"Fuentes encontradas: {len(respuesta.sources)}")

        for i, source in enumerate(respuesta.sources):
            print(f"\n--- Fuente {i + 1} ---")
            print(f"ID: {source.id}")
            print(f"Score: {source.score:.4f}")
            if source.meta:
                print(f"Metadatos: {source.meta.get('tema', 'N/A')}")
            if source.content:
                preview = source.content[:150] + "..." if len(source.content) > 150 else source.content
                print(f"Contenido: {preview}")

        if respuesta.usage:
            print(f"\nğŸ“Š Uso de tokens: {respuesta.usage}")

        # Hacer otra pregunta
        print("\n" + "=" * 50)
        print("Segunda consulta...")
        print("=" * 50)

        respuesta2 = client.ask(collection="mi_coleccion", question="Â¿QuÃ© es Machine Learning?", k=2)

        print("\nRESPUESTA 2:")
        print(respuesta2.answer)
        print(f"\nFuentes: {len(respuesta2.sources)}")

        # Cerrar cliente
        client.close()
        print("\nPrueba completada exitosamente")

    except Exception as e:
        print(f"âŒ Error durante la prueba: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    # Para pruebas sÃ­ncronas, puedes usar directamente:
    # main()
    asyncio.run(main())
